{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1c97880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauri\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# for text preprocessing\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# import numpy for matrix operation\n",
    "import numpy as np\n",
    "\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "07419f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauri\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2c90208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d8df4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = 'I want to watch a movie this weekend.How are you'\n",
    "D2 =  'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.'\n",
    "D3 =  'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.'\n",
    "D4 =  'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!'\n",
    "D5 =  'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe467a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [D1, D2, D3, D4, D5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "abaac5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I want to watch a movie this weekend.How are you',\n",
       " 'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.',\n",
       " 'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.',\n",
       " 'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!',\n",
       " 'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "88125ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop loss words \n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# punctuation \n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# lemmatization\n",
    "lemma = WordNetLemmatizer() \n",
    "\n",
    "# One function for all the steps:\n",
    "def clean(doc):\n",
    "    \n",
    "    # convert text into lower case + split into words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    \n",
    "    # remove any stop words present\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)  \n",
    "    \n",
    "    # remove punctuations + normalize the text\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dba66928",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus = [clean(doc).split()  for doc in corpus]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bec4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = 'I want to watch a movie this weekend.How are you this weekend'\n",
    "D2 =  'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.'\n",
    "D3 =  'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.'\n",
    "D4 =  'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!'\n",
    "D5 =  'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dbec5d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'watch', 'movie', 'weekendhow'],\n",
       " ['went',\n",
       "  'shopping',\n",
       "  'yesterday',\n",
       "  'new',\n",
       "  'zealand',\n",
       "  'world',\n",
       "  'test',\n",
       "  'championship',\n",
       "  'beating',\n",
       "  'india',\n",
       "  'eight',\n",
       "  'wicket',\n",
       "  'southampton'],\n",
       " ['don’t',\n",
       "  'watch',\n",
       "  'cricket',\n",
       "  'netflix',\n",
       "  'amazon',\n",
       "  'prime',\n",
       "  'good',\n",
       "  'movie',\n",
       "  'watch'],\n",
       " ['movie',\n",
       "  'nice',\n",
       "  'way',\n",
       "  'chill',\n",
       "  'however',\n",
       "  'time',\n",
       "  'would',\n",
       "  'like',\n",
       "  'paint',\n",
       "  'read',\n",
       "  'good',\n",
       "  'book',\n",
       "  'it’s',\n",
       "  'long'],\n",
       " ['blueberry',\n",
       "  'milkshake',\n",
       "  'good',\n",
       "  'try',\n",
       "  'reading',\n",
       "  'dr',\n",
       "  'joe',\n",
       "  'dispenza’s',\n",
       "  'book',\n",
       "  'work',\n",
       "  'gamechanger',\n",
       "  'book']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "172e2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = corpora.Dictionary(clean_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a979813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x2348f78ea60>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff0ea359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<44 unique tokens: ['movie', 'want', 'watch', 'weekend', 'beating']...>\n"
     ]
    }
   ],
   "source": [
    "print(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e513031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dict_.doc2bow(i) for i in clean_corpus] #passing each doc to doc matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47f728ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0388ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in clean_corpus:\n",
    "    new = dict_.doc2bow(i)\n",
    "    doc_term_matrix2.append(new)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d428a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = 'I want want to watch a movie this weekend.'\n",
    "D2 =  'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.'\n",
    "D3 =  'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.'\n",
    "D4 =  'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!'\n",
    "D5 =  'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84a59d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1)],\n",
       " [(0, 1), (2, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)],\n",
       " [(0, 1),\n",
       "  (21, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1)],\n",
       " [(21, 1),\n",
       "  (24, 2),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1)]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " doc_term_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0711408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1)],\n",
       " [(0, 1), (2, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)],\n",
       " [(0, 1),\n",
       "  (20, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1)],\n",
       " [(20, 1),\n",
       "  (23, 2),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1)]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix  #[(0, 1), (1, 1), (2, 1), (3, 1)]--- this is one document  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a20bd",
   "metadata": {},
   "source": [
    "doc_term_matrix  #[(0, 1), (1, 1), (2, 1), (3, 1)]--- this is one document  ['went',\n",
    "  'shopping',\n",
    "  'yesterday',\n",
    "  'new',\n",
    "  'zealand',\n",
    "  'world',\n",
    "  'test',\n",
    "  'championship',\n",
    "  'beating',\n",
    "  'india',\n",
    "  'eight',\n",
    "  'wicket',\n",
    "  'southampton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f32f05b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1)],\n",
       " [(0, 1), (2, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)],\n",
       " [(0, 1),\n",
       "  (21, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1)],\n",
       " [(21, 1),\n",
       "  (24, 2),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1)]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix #[['want', 'watch', 'movie', 'weekendhow', 'weekend'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cc264a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-d1bfa03e6988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-d1bfa03e6988>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-d1bfa03e6988>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "[[(dict_[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9cefa68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "936f5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=6, id2word = dict_, passes=1, random_state=0, eval_every=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b45981f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ldamodel.print_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f2cac0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.057*\"world\" + 0.057*\"southampton\" + 0.057*\"zealand\" + 0.057*\"championship\" + 0.057*\"eight\" + 0.057*\"new\" + 0.057*\"wicket\" + 0.057*\"shopping\" + 0.057*\"test\" + 0.057*\"beating\"'),\n",
       " (1,\n",
       "  '0.023*\"watch\" + 0.023*\"movie\" + 0.023*\"want\" + 0.023*\"good\" + 0.023*\"weekendhow\" + 0.023*\"book\" + 0.023*\"prime\" + 0.023*\"netflix\" + 0.023*\"amazon\" + 0.023*\"joe\"'),\n",
       " (2,\n",
       "  '0.095*\"book\" + 0.065*\"good\" + 0.035*\"movie\" + 0.035*\"it’s\" + 0.035*\"dispenza’s\" + 0.035*\"paint\" + 0.035*\"time\" + 0.035*\"way\" + 0.035*\"long\" + 0.035*\"reading\"'),\n",
       " (3,\n",
       "  '0.023*\"movie\" + 0.023*\"watch\" + 0.023*\"want\" + 0.023*\"weekendhow\" + 0.023*\"good\" + 0.023*\"book\" + 0.023*\"prime\" + 0.023*\"went\" + 0.023*\"would\" + 0.023*\"yesterday\"'),\n",
       " (4,\n",
       "  '0.103*\"movie\" + 0.103*\"weekendhow\" + 0.103*\"watch\" + 0.103*\"want\" + 0.015*\"good\" + 0.015*\"book\" + 0.015*\"went\" + 0.015*\"amazon\" + 0.015*\"prime\" + 0.015*\"cricket\"'),\n",
       " (5,\n",
       "  '0.133*\"watch\" + 0.071*\"movie\" + 0.071*\"good\" + 0.071*\"cricket\" + 0.071*\"don’t\" + 0.071*\"netflix\" + 0.071*\"amazon\" + 0.071*\"prime\" + 0.010*\"weekendhow\" + 0.010*\"want\"')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9c506017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " '0.023*\"watch\" + 0.023*\"movie\" + 0.023*\"want\" + 0.023*\"good\" + 0.023*\"weekendhow\" + 0.023*\"book\" + 0.023*\"prime\" + 0.023*\"netflix\" + 0.023*\"amazon\" + 0.023*\"joe\"')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8392f747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('world', 0.057357166),\n",
       "   ('southampton', 0.05735527),\n",
       "   ('zealand', 0.05735216),\n",
       "   ('championship', 0.057349794),\n",
       "   ('eight', 0.057347685),\n",
       "   ('new', 0.057347022),\n",
       "   ('wicket', 0.057346858),\n",
       "   ('shopping', 0.05734601),\n",
       "   ('test', 0.05734443),\n",
       "   ('beating', 0.057344135)]),\n",
       " (1,\n",
       "  [('want', 0.022824783),\n",
       "   ('watch', 0.022806449),\n",
       "   ('movie', 0.022777785),\n",
       "   ('good', 0.02276728),\n",
       "   ('book', 0.02273984),\n",
       "   ('weekend', 0.022732534),\n",
       "   ('prime', 0.022730265),\n",
       "   ('netflix', 0.022729326),\n",
       "   ('amazon', 0.022728752),\n",
       "   ('joe', 0.022724386)]),\n",
       " (2,\n",
       "  [('book', 0.09498497),\n",
       "   ('good', 0.06498759),\n",
       "   ('movie', 0.035006292),\n",
       "   ('it’s', 0.034998365),\n",
       "   ('dispenza’s', 0.034997724),\n",
       "   ('paint', 0.034995466),\n",
       "   ('time', 0.034995105),\n",
       "   ('way', 0.034994997),\n",
       "   ('long', 0.034994222),\n",
       "   ('reading', 0.034993693)]),\n",
       " (3,\n",
       "  [('want', 0.022817224),\n",
       "   ('movie', 0.022788346),\n",
       "   ('watch', 0.022781398),\n",
       "   ('good', 0.022758443),\n",
       "   ('book', 0.02275271),\n",
       "   ('weekend', 0.022750042),\n",
       "   ('prime', 0.022739394),\n",
       "   ('went', 0.022734886),\n",
       "   ('would', 0.022729926),\n",
       "   ('yesterday', 0.02272742)]),\n",
       " (4,\n",
       "  [('want', 0.17535518),\n",
       "   ('movie', 0.09450522),\n",
       "   ('weekend', 0.09448079),\n",
       "   ('watch', 0.09446633),\n",
       "   ('good', 0.013550998),\n",
       "   ('book', 0.013546306),\n",
       "   ('went', 0.013534168),\n",
       "   ('amazon', 0.013534133),\n",
       "   ('prime', 0.013533858),\n",
       "   ('cricket', 0.013533606)]),\n",
       " (5,\n",
       "  [('watch', 0.13258965),\n",
       "   ('movie', 0.07139638),\n",
       "   ('good', 0.07139227),\n",
       "   ('cricket', 0.07138576),\n",
       "   ('don’t', 0.071383834),\n",
       "   ('netflix', 0.0713703),\n",
       "   ('amazon', 0.071367316),\n",
       "   ('prime', 0.07135696),\n",
       "   ('want', 0.010241937),\n",
       "   ('book', 0.010228222)])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.show_topics(num_words=10, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74de640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = ldamodel.get_document_topics(doc_term_matrix,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5266fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_corpus= ldamodel[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fc41ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1e77216a130>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa3bfce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(all_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b23b144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2f298f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-fec8107d8fb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmajor_topic\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_topics_numpy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf_orig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'major_lda_topic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmajor_topic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_orig' is not defined"
     ]
    }
   ],
   "source": [
    "all_topics_csr= gensim.matutils.corpus2csc(all_topics)\n",
    "all_topics_numpy= all_topics_csr.T.toarray()\n",
    "\n",
    "major_topic= [np.argmax(arr) for arr in all_topics_numpy]\n",
    "df_orig['major_lda_topic']= major_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "284902ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.4427748999629768\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "coherence_model_lda=CoherenceModel(model=ldamodel,texts=clean_corpus,dictionary=dict_,coherence='c_v')\n",
    "coherence_lda=coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score:',coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "90423439",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics=ldamodel.top_topics(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "42d11217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4dea3089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauri\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-c98f35067c2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtopics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'topics' is not defined"
     ]
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1e0d902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  '0.023*\"want\" + 0.023*\"watch\" + 0.023*\"movie\" + 0.023*\"good\" + 0.023*\"book\" + 0.023*\"weekend\" + 0.023*\"prime\" + 0.023*\"netflix\" + 0.023*\"amazon\" + 0.023*\"joe\"'),\n",
       " (2,\n",
       "  '0.095*\"book\" + 0.065*\"good\" + 0.035*\"movie\" + 0.035*\"it’s\" + 0.035*\"dispenza’s\" + 0.035*\"paint\" + 0.035*\"time\" + 0.035*\"way\" + 0.035*\"long\" + 0.035*\"reading\"'),\n",
       " (0,\n",
       "  '0.057*\"world\" + 0.057*\"southampton\" + 0.057*\"zealand\" + 0.057*\"championship\" + 0.057*\"eight\" + 0.057*\"new\" + 0.057*\"wicket\" + 0.057*\"shopping\" + 0.057*\"test\" + 0.057*\"beating\"'),\n",
       " (5,\n",
       "  '0.133*\"watch\" + 0.071*\"movie\" + 0.071*\"good\" + 0.071*\"cricket\" + 0.071*\"don’t\" + 0.071*\"netflix\" + 0.071*\"amazon\" + 0.071*\"prime\" + 0.010*\"want\" + 0.010*\"book\"'),\n",
       " (3,\n",
       "  '0.023*\"want\" + 0.023*\"movie\" + 0.023*\"watch\" + 0.023*\"good\" + 0.023*\"book\" + 0.023*\"weekend\" + 0.023*\"prime\" + 0.023*\"went\" + 0.023*\"would\" + 0.023*\"yesterday\"')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.show_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fd12699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.057*\"world\" + 0.057*\"southampton\" + 0.057*\"zealand\" + 0.057*\"championship\" + 0.057*\"eight\" + 0.057*\"new\" + 0.057*\"wicket\" + 0.057*\"shopping\" + 0.057*\"test\" + 0.057*\"beating\"\n",
      "0.023*\"want\" + 0.023*\"watch\" + 0.023*\"movie\" + 0.023*\"good\" + 0.023*\"book\" + 0.023*\"weekend\" + 0.023*\"prime\" + 0.023*\"netflix\" + 0.023*\"amazon\" + 0.023*\"joe\"\n",
      "0.095*\"book\" + 0.065*\"good\" + 0.035*\"movie\" + 0.035*\"it’s\" + 0.035*\"dispenza’s\" + 0.035*\"paint\" + 0.035*\"time\" + 0.035*\"way\" + 0.035*\"long\" + 0.035*\"reading\"\n",
      "0.023*\"want\" + 0.023*\"movie\" + 0.023*\"watch\" + 0.023*\"good\" + 0.023*\"book\" + 0.023*\"weekend\" + 0.023*\"prime\" + 0.023*\"went\" + 0.023*\"would\" + 0.023*\"yesterday\"\n",
      "0.175*\"want\" + 0.095*\"movie\" + 0.094*\"weekend\" + 0.094*\"watch\" + 0.014*\"good\" + 0.014*\"book\" + 0.014*\"went\" + 0.014*\"amazon\" + 0.014*\"prime\" + 0.014*\"cricket\"\n",
      "0.133*\"watch\" + 0.071*\"movie\" + 0.071*\"good\" + 0.071*\"cricket\" + 0.071*\"don’t\" + 0.071*\"netflix\" + 0.071*\"amazon\" + 0.071*\"prime\" + 0.010*\"want\" + 0.010*\"book\"\n"
     ]
    }
   ],
   "source": [
    "y = np.array(ldamodel.show_topics(num_topics=6, num_words=10))\n",
    "for i in y[:,1]:\n",
    "    #if i != '%d':\n",
    "      #  print([str(word) for word in i])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12cc00fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauri\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis.gensim_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-b860832df245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim_models\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgensimvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.gensim_models'"
     ]
    }
   ],
   "source": [
    "import  pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis= pyLDAvis.gensim.prepare(ldamodel,doc_term_matrix,dict_)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a5905187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyldavis\n",
      "  Using cached pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: joblib in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyldavis) (1.0.1)\n",
      "Requirement already satisfied: numexpr in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyldavis) (2.7.3)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyldavis) (1.7.3)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyldavis) (1.4.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyldavis) (0.24.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyldavis) (4.2.0)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyldavis) (2.11.3)\n",
      "Requirement already satisfied: future in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyldavis) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyldavis) (1.21.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyldavis) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyldavis) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyldavis) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyldavis) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from gensim->pyldavis) (5.2.1)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from gensim->pyldavis) (0.29.28)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from jinja2->pyldavis) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from scikit-learn->pyldavis) (2.1.0)\n",
      "Building wheels for collected packages: pyldavis, sklearn\n",
      "  Building wheel for pyldavis (PEP 517): started\n",
      "  Building wheel for pyldavis (PEP 517): finished with status 'done'\n",
      "  Created wheel for pyldavis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136882 sha256=491a67e17649de69f411147a50eb96b590ef94d0e366140e6442ef76bb7135b0\n",
      "  Stored in directory: c:\\users\\gauri\\appdata\\local\\pip\\cache\\wheels\\90\\61\\ec\\9dbe9efc3acf9c4e37ba70fbbcc3f3a0ebd121060aa593181a\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=912eaf091bc4f14dbcb7b28677fa78ffc3519937375d18a527df3f179c477bbf\n",
      "  Stored in directory: c:\\users\\gauri\\appdata\\local\\pip\\cache\\wheels\\22\\0b\\40\\fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built pyldavis sklearn\n",
      "Installing collected packages: sklearn, funcy, pyldavis\n",
      "Successfully installed funcy-1.17 pyldavis-3.3.1 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f1064d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauri\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis==2.1.2\n",
      "  Downloading pyLDAvis-2.1.2.tar.gz (1.6 MB)\n",
      "Requirement already satisfied: wheel>=0.23.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyLDAvis==2.1.2) (0.36.2)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyLDAvis==2.1.2) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyLDAvis==2.1.2) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyLDAvis==2.1.2) (1.4.3)\n",
      "Requirement already satisfied: joblib>=0.8.4 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyLDAvis==2.1.2) (1.0.1)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyLDAvis==2.1.2) (2.11.3)\n",
      "Requirement already satisfied: numexpr in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyLDAvis==2.1.2) (2.7.3)\n",
      "Requirement already satisfied: pytest in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyLDAvis==2.1.2) (6.2.4)\n",
      "Requirement already satisfied: future in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyLDAvis==2.1.2) (0.18.2)\n",
      "Requirement already satisfied: funcy in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pyLDAvis==2.1.2) (1.17)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.17.0->pyLDAvis==2.1.2) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (21.2.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (21.0)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (1.10.0)\n",
      "Requirement already satisfied: toml in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (0.10.2)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from packaging->pytest->pyLDAvis==2.1.2) (2.4.7)\n",
      "Building wheels for collected packages: pyLDAvis\n",
      "  Building wheel for pyLDAvis (setup.py): started\n",
      "  Building wheel for pyLDAvis (setup.py): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=3d47139708115d084c3e348849da7054bda108ae7231172a4ee36cce3450b4ce\n",
      "  Stored in directory: c:\\users\\gauri\\appdata\\local\\pip\\cache\\wheels\\31\\8c\\a0\\24a443892f2134e691d59c8c6c35e19821e02f85e49871f8fd\n",
      "Successfully built pyLDAvis\n",
      "Installing collected packages: pyLDAvis\n",
      "  Attempting uninstall: pyLDAvis\n",
      "    Found existing installation: pyLDAvis 3.3.1\n",
      "    Uninstalling pyLDAvis-3.3.1:\n",
      "      Successfully uninstalled pyLDAvis-3.3.1\n",
      "Successfully installed pyLDAvis-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(0,\n",
    "  [('world', 0.057357166),\n",
    "   ('southampton', 0.05735527),\n",
    "   ('zealand', 0.05735216),\n",
    "   ('championship', 0.057349794),\n",
    "   ('eight', 0.057347685),\n",
    "   ('new', 0.057347022),\n",
    "   ('wicket', 0.057346858),\n",
    "   ('shopping', 0.05734601),\n",
    "   ('test', 0.05734443),\n",
    "   ('beating', 0.057344135)]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "98a850d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 0.86091626), (5, 0.027946794), (2, 0.027800994), (1, 0.027778894), (3, 0.027778894), (0, 0.027778193)] 0\n",
      "[(0, 0.94047296), (1, 0.011905747), (3, 0.011905747), (4, 0.011905346), (5, 0.011905208), (2, 0.011904997)] 1\n",
      "[(5, 0.91657627), (4, 0.016720848), (2, 0.016700564), (1, 0.016667642), (3, 0.016667642), (0, 0.01666703)] 2\n",
      "[(2, 0.94438946), (5, 0.0111377165), (4, 0.011136239), (1, 0.011112486), (3, 0.011112486), (0, 0.011111623)] 3\n",
      "[(2, 0.935882), (5, 0.012831637), (1, 0.012821968), (3, 0.012821968), (4, 0.012821379), (0, 0.012821052)] 4\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(ldamodel[doc_term_matrix]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f44da05e",
   "metadata": {},
   "outputs": [],
   "source": [
    " wp = ldamodel.show_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ffe06e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 0.057357166),\n",
       " ('southampton', 0.05735527),\n",
       " ('zealand', 0.05735216),\n",
       " ('championship', 0.057349794),\n",
       " ('eight', 0.057347685),\n",
       " ('new', 0.057347022),\n",
       " ('wicket', 0.057346858),\n",
       " ('shopping', 0.05734601),\n",
       " ('test', 0.05734443),\n",
       " ('beating', 0.057344135)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aac0b785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 0.057357166),\n",
       " ('southampton', 0.05735527),\n",
       " ('zealand', 0.05735216),\n",
       " ('championship', 0.057349794),\n",
       " ('eight', 0.057347685),\n",
       " ('new', 0.057347022),\n",
       " ('wicket', 0.057346858),\n",
       " ('shopping', 0.05734601),\n",
       " ('test', 0.05734443),\n",
       " ('beating', 0.057344135)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb59df42",
   "metadata": {},
   "outputs": [],
   "source": [
    " sent_topics_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb8203d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-ac1c59be83b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtopic_keywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msent_topics_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_topics_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprop_topic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_keywords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'topic_num' is not defined"
     ]
    }
   ],
   "source": [
    "topic_keywords = \", \".join([word for word, prop in wp])\n",
    "sent_topics_df = sent_topics_df.append(pd.Series([int(0)pic_num, round(prop_topic,4), topic_keywords]), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a4651a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "contents = pd.Series(clean_corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7286fe46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  [want, want, watch, movie, weekend]\n",
       "1    [went, shopping, yesterday, new, zealand, worl...\n",
       "2    [don’t, watch, cricket, netflix, amazon, prime...\n",
       "3    [movie, nice, way, chill, however, time, would...\n",
       "4    [blueberry, milkshake, good, try, reading, dr,...\n",
       "dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217fcff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=ldamodel, corpus=doc_term_matrix, texts=clean_corpus):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    #sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodeltoo0, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30045ff0",
   "metadata": {},
   "source": [
    "Topic coherence measures the average similarity between top words having the highest weights in a topic i.e relative distance between the top words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5bbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, num_topics_range,alpha_range):\n",
    "    coherence_values=[]\n",
    "    model_list=[]\n",
    "    for alpha in alpha_range:\n",
    "        for num_topics in num_topics_range:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ldamodel,texts=clean_corpus,dictionary=dict_,coherence='c_v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a5114fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = Lda(doc_term_matrix, num_topics=6, id2word = dict_, passes=1, random_state=0, eval_every=None)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f97f4032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x1e77a11c0a0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "04774594",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_lda=CoherenceModel(model=model_list[2], texts=clean_corpus, dictionary=dict_, coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c39dfde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.coherencemodel.CoherenceModel at 0x1e77a19b4c0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9d974736",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_value=coherence_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8f90ca40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4427748999629768"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "27c71478",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dict_, corpus=doc_term_matrix, texts=clean_corpus, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "abbfb537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4427748999629768,\n",
       " 0.4427748999629768,\n",
       " 0.4427748999629768,\n",
       " 0.4427748999629768,\n",
       " 0.4427748999629768,\n",
       " 0.4427748999629768,\n",
       " 0.4427748999629768]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8018ac7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<gensim.models.ldamodel.LdaModel at 0x1e772809a60>,\n",
       " <gensim.models.ldamodel.LdaModel at 0x1e771c82fa0>,\n",
       " <gensim.models.ldamodel.LdaModel at 0x1e772809d90>,\n",
       " <gensim.models.ldamodel.LdaModel at 0x1e772809af0>,\n",
       " <gensim.models.ldamodel.LdaModel at 0x1e7720c9e50>,\n",
       " <gensim.models.ldamodel.LdaModel at 0x1e7727f51f0>,\n",
       " <gensim.models.ldamodel.LdaModel at 0x1e77216a1c0>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d3db8929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128aa42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb9637a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertopic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1d2f5f7c9d67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f6d9d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Downloading bertopic-0.12.0-py2.py3-none-any.whl (90 kB)\n",
      "Collecting numpy>=1.20.0\n",
      "  Downloading numpy-1.23.3-cp38-cp38-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from bertopic) (1.4.0)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
      "Collecting hdbscan>=0.8.28\n",
      "  Downloading hdbscan-0.8.28.tar.gz (5.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: pyyaml<6.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from bertopic) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from bertopic) (4.61.2)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from bertopic) (0.24.2)\n",
      "Collecting plotly>=4.7.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\gauri\\anaconda3\\python.exe' 'C:\\Users\\gauri\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\gauri\\AppData\\Local\\Temp\\tmpfu57aus6'\n",
      "       cwd: C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-install-gyh4i748\\hdbscan_26b7d4d2804c43919d7c97783fb79512\n",
      "  Complete output (40 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-38\n",
      "  creating build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\flat.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\plots.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\prediction.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\validity.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  copying hdbscan\\__init__.py -> build\\lib.win-amd64-cpython-38\\hdbscan\n",
      "  creating build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_prediction_utils.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-cpython-38\\hdbscan\\tests\n",
      "  running build_ext\n",
      "  cythoning hdbscan/_hdbscan_tree.pyx to hdbscan\\_hdbscan_tree.c\n",
      "  cythoning hdbscan/_hdbscan_linkage.pyx to hdbscan\\_hdbscan_linkage.c\n",
      "  cythoning hdbscan/_hdbscan_boruvka.pyx to hdbscan\\_hdbscan_boruvka.c\n",
      "  cythoning hdbscan/_hdbscan_reachability.pyx to hdbscan\\_hdbscan_reachability.c\n",
      "  cythoning hdbscan/_prediction_utils.pyx to hdbscan\\_prediction_utils.c\n",
      "  cythoning hdbscan/dist_metrics.pyx to hdbscan\\dist_metrics.c\n",
      "  building 'hdbscan._hdbscan_tree' extension\n",
      "  C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-build-env-c449zqax\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-install-gyh4i748\\hdbscan_26b7d4d2804c43919d7c97783fb79512\\hdbscan\\_hdbscan_tree.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-build-env-c449zqax\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-install-gyh4i748\\hdbscan_26b7d4d2804c43919d7c97783fb79512\\hdbscan\\_hdbscan_linkage.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-build-env-c449zqax\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-install-gyh4i748\\hdbscan_26b7d4d2804c43919d7c97783fb79512\\hdbscan\\_hdbscan_boruvka.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-build-env-c449zqax\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-install-gyh4i748\\hdbscan_26b7d4d2804c43919d7c97783fb79512\\hdbscan\\_hdbscan_reachability.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-build-env-c449zqax\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-install-gyh4i748\\hdbscan_26b7d4d2804c43919d7c97783fb79512\\hdbscan\\_prediction_utils.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-build-env-c449zqax\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\gauri\\AppData\\Local\\Temp\\pip-install-gyh4i748\\hdbscan_26b7d4d2804c43919d7c97783fb79512\\hdbscan\\dist_metrics.pxd\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for hdbscan"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (1.0.1)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (0.29.28)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.28->bertopic) (1.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2021.1)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.1.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.12.1-cp38-cp38-win_amd64.whl (161.9 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.13.1-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: nltk in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.6.2)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.10.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.12)\n",
      "Requirement already satisfied: requests in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.4)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.7.6)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.53.1)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.36.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (52.0.0.post20210125)\n",
      "Requirement already satisfied: click in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.6.15)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\gauri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.0.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.2.0-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
      "  Building wheel for hdbscan (PEP 517): started\n",
      "  Building wheel for hdbscan (PEP 517): finished with status 'error'\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125918 sha256=967f51427f4048116abe97fed041be0490d5cc9e8fff3323789a3e14fbe53d86\n",
      "  Stored in directory: c:\\users\\gauri\\appdata\\local\\pip\\cache\\wheels\\5e\\6f\\8c\\d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
      "  Building wheel for umap-learn (setup.py): started\n",
      "  Building wheel for umap-learn (setup.py): finished with status 'done'\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82820 sha256=8bdf63b44615c04f74e6bb8851933fb3143b9355c66940e3711a44d1ea6e4338\n",
      "  Stored in directory: c:\\users\\gauri\\appdata\\local\\pip\\cache\\wheels\\a9\\3a\\67\\06a8950e053725912e6a8c42c4a3a241410f6487b8402542ea\n",
      "  Building wheel for pynndescent (setup.py): started\n",
      "  Building wheel for pynndescent (setup.py): finished with status 'done'\n",
      "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54271 sha256=33dd07c1e9d8c8911430645327f7dc7a2387bd87d6e7c919cacd11dfa011664b\n",
      "  Stored in directory: c:\\users\\gauri\\appdata\\local\\pip\\cache\\wheels\\1b\\38\\fe\\99e22fbae88abd1c5e8d99253cba6d1c590cc7a94408bff3bf\n",
      "Successfully built sentence-transformers umap-learn pynndescent\n",
      "Failed to build hdbscan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/win-64::anaconda-client==1.8.0=py38haa95532_0\n",
      "  - defaults/win-64::anaconda==custom=py38_1\n",
      "  - defaults/win-64::anaconda-navigator==2.0.3=py38_0\n",
      "  - defaults/noarch::anaconda-project==0.10.0=pyhd3eb1b0_0\n",
      "  - defaults/win-64::astropy==4.2.1=py38h2bbff1b_1\n",
      "  - defaults/win-64::bkcharts==0.2=py38_0\n",
      "  - defaults/noarch::black==19.10b0=py_0\n",
      "  - defaults/win-64::bokeh==2.3.3=py38haa95532_0\n",
      "  - defaults/win-64::bottleneck==1.3.2=py38h2a96729_1\n",
      "  - defaults/noarch::click==8.0.1=pyhd3eb1b0_0\n",
      "  - defaults/noarch::conda-repo-cli==1.0.4=pyhd3eb1b0_0\n",
      "  - defaults/noarch::conda-verify==3.4.2=py_1\n",
      "  - defaults/noarch::dask==2021.6.2=pyhd3eb1b0_0\n",
      "  - defaults/win-64::distributed==2021.6.2=py38haa95532_0\n",
      "  - defaults/noarch::flake8==3.9.0=pyhd3eb1b0_0\n",
      "  - conda-forge/label/gcc7/noarch::flasgger==0.9.1=py_0\n",
      "  - defaults/noarch::flask==1.1.2=pyhd3eb1b0_0\n",
      "  - defaults/noarch::google-auth-oauthlib==0.4.4=pyhd3eb1b0_0\n",
      "  - defaults/win-64::h5py==2.10.0=py38h5e291fa_0\n",
      "  - defaults/win-64::imagecodecs==2021.8.26=py38ha1f97ea_0\n",
      "  - defaults/noarch::imageio==2.9.0=pyhd3eb1b0_0\n",
      "  - defaults/noarch::importlib_metadata==3.10.0=hd3eb1b0_0\n",
      "  - defaults/noarch::ipywidgets==7.6.3=pyhd3eb1b0_1\n",
      "  - defaults/noarch::jsonschema==3.2.0=py_2\n",
      "  - defaults/win-64::jupyter==1.0.0=py38_7\n",
      "  - defaults/noarch::jupyterlab==3.0.14=pyhd3eb1b0_1\n",
      "  - defaults/noarch::jupyterlab_server==2.4.0=pyhd3eb1b0_0\n",
      "  - defaults/win-64::jupyter_server==1.4.1=py38haa95532_0\n",
      "  - defaults/noarch::keras-applications==1.0.8=py_1\n",
      "  - defaults/noarch::keras-preprocessing==1.1.2=pyhd3eb1b0_0\n",
      "  - defaults/win-64::keyring==23.0.1=py38haa95532_0\n",
      "  - defaults/win-64::markdown==3.3.4=py38haa95532_0\n",
      "  - defaults/win-64::matplotlib==3.3.4=py38haa95532_0\n",
      "  - defaults/win-64::matplotlib-base==3.3.4=py38h49ac443_0\n",
      "  - defaults/win-64::mkl_fft==1.3.0=py38h277e83a_2\n",
      "  - defaults/win-64::mkl_random==1.2.1=py38hf11a4ad_2\n",
      "  - defaults/noarch::nbclassic==0.2.6=pyhd3eb1b0_0\n",
      "  - defaults/noarch::nbclient==0.5.3=pyhd3eb1b0_0\n",
      "  - defaults/win-64::nbconvert==6.1.0=py38haa95532_0\n",
      "  - defaults/noarch::nbformat==5.1.3=pyhd3eb1b0_0\n",
      "  - defaults/noarch::nltk==3.6.2=pyhd3eb1b0_0\n",
      "  - defaults/win-64::notebook==6.4.0=py38haa95532_0\n",
      "  - defaults/win-64::numba==0.53.1=py38hf11a4ad_0\n",
      "  - defaults/win-64::numexpr==2.7.3=py38hb80d3ca_1\n",
      "  - defaults/win-64::numpy==1.20.2=py38ha4e8547_0\n",
      "  - defaults/noarch::opt_einsum==3.3.0=pyhd3eb1b0_1\n",
      "  - defaults/win-64::patsy==0.5.1=py38_0\n",
      "  - defaults/win-64::pyerfa==2.0.0=py38h2bbff1b_0\n",
      "  - defaults/noarch::pyls-black==0.4.6=hd3eb1b0_0\n",
      "  - defaults/noarch::pyls-spyder==0.3.2=pyhd3eb1b0_0\n",
      "  - defaults/win-64::pytables==3.6.1=py38ha5be198_0\n",
      "  - defaults/noarch::python-language-server==0.36.2=pyhd3eb1b0_0\n",
      "  - defaults/win-64::pywavelets==1.1.1=py38he774522_2\n",
      "  - defaults/win-64::scikit-image==0.18.1=py38hf11a4ad_0\n",
      "  - defaults/win-64::scikit-learn==0.24.2=py38hf11a4ad_1\n",
      "  - defaults/noarch::seaborn==0.11.1=pyhd3eb1b0_0\n",
      "  - defaults/win-64::spyder==4.2.5=py38haa95532_0\n",
      "  - defaults/win-64::statsmodels==0.12.2=py38h2bbff1b_0\n",
      "  - defaults/win-64::tensorboard==2.8.0=py38haa95532_0\n",
      "  - defaults/win-64::tensorflow==2.3.0=mkl_py38h8c0d9a2_0\n",
      "  - defaults/win-64::tensorflow-base==2.3.0=eigen_py38h75a453f_0\n",
      "  - defaults/noarch::tensorflow-estimator==2.6.0=pyh7b7c402_0\n",
      "  - defaults/noarch::tifffile==2021.4.8=pyhd3eb1b0_2\n",
      "  - defaults/win-64::widgetsnbextension==3.5.1=py38_0\n",
      "  - defaults/win-64::_anaconda_depends==2020.07=py38_0\n",
      "  - defaults/win-64::_ipyw_jlab_nb_ext_conf==0.1.0=py38_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\gauri\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - hdbscan\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2022.9.14  |       h5b45459_0         189 KB  conda-forge\n",
      "    certifi-2022.9.14          |     pyhd8ed1ab_0         156 KB  conda-forge\n",
      "    conda-4.14.0               |   py38haa244fe_0         1.0 MB  conda-forge\n",
      "    hdbscan-0.8.28             |   py38h6f4d8f0_1         496 KB  conda-forge\n",
      "    mkl-2021.4.0               |     haa95532_640       114.9 MB\n",
      "    numpy-1.21.5               |   py38h7a0a035_3          25 KB\n",
      "    numpy-base-1.21.5          |   py38hca35cd5_3         4.4 MB\n",
      "    openssl-1.1.1q             |       h8ffe710_0         5.8 MB  conda-forge\n",
      "    pandas-1.4.3               |   py38hcc40339_0        11.0 MB  conda-forge\n",
      "    scipy-1.7.3                |   py38h0a974cb_0        13.9 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       151.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  hdbscan            conda-forge/win-64::hdbscan-0.8.28-py38h6f4d8f0_1\n",
      "  importlib-metadata pkgs/main/win-64::importlib-metadata-3.10.0-py38haa95532_0\n",
      "  numpy-base         pkgs/main/win-64::numpy-base-1.21.5-py38hca35cd5_3\n",
      "  pandas             conda-forge/win-64::pandas-1.4.3-py38hcc40339_0\n",
      "  python_abi         conda-forge/win-64::python_abi-3.8-2_cp38\n",
      "  scipy              pkgs/main/win-64::scipy-1.7.3-py38h0a974cb_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2022.07.19~ --> conda-forge::ca-certificates-2022.9.14-h5b45459_0\n",
      "  certifi            pkgs/main/win-64::certifi-2022.6.15-p~ --> conda-forge/noarch::certifi-2022.9.14-pyhd8ed1ab_0\n",
      "  mkl                                 2021.2.0-haa95532_296 --> 2021.4.0-haa95532_640\n",
      "  numpy                               1.20.2-py38ha4e8547_0 --> 1.21.5-py38h7a0a035_3\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda              pkgs/main::conda-4.14.0-py38haa95532_0 --> conda-forge::conda-4.14.0-py38haa244fe_0\n",
      "  openssl              pkgs/main::openssl-1.1.1q-h2bbff1b_0 --> conda-forge::openssl-1.1.1q-h8ffe710_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "pandas-1.4.3         | 11.0 MB   |            |   0% \n",
      "pandas-1.4.3         | 11.0 MB   |            |   0% \n",
      "pandas-1.4.3         | 11.0 MB   | #2         |  13% \n",
      "pandas-1.4.3         | 11.0 MB   | ######     |  60% \n",
      "pandas-1.4.3         | 11.0 MB   | ########5  |  86% \n",
      "pandas-1.4.3         | 11.0 MB   | ########## | 100% \n",
      "\n",
      "scipy-1.7.3          | 13.9 MB   |            |   0% \n",
      "scipy-1.7.3          | 13.9 MB   |            |   0% \n",
      "scipy-1.7.3          | 13.9 MB   | #2         |  13% \n",
      "scipy-1.7.3          | 13.9 MB   | #####      |  50% \n",
      "scipy-1.7.3          | 13.9 MB   | #######7   |  78% \n",
      "scipy-1.7.3          | 13.9 MB   | ########## | 100% \n",
      "\n",
      "mkl-2021.4.0         | 114.9 MB  |            |   0% \n",
      "mkl-2021.4.0         | 114.9 MB  |            |   1% \n",
      "mkl-2021.4.0         | 114.9 MB  | 5          |   6% \n",
      "mkl-2021.4.0         | 114.9 MB  | #1         |  11% \n",
      "mkl-2021.4.0         | 114.9 MB  | #6         |  16% \n",
      "mkl-2021.4.0         | 114.9 MB  | ##         |  21% \n",
      "mkl-2021.4.0         | 114.9 MB  | ##5        |  26% \n",
      "mkl-2021.4.0         | 114.9 MB  | ###1       |  32% \n",
      "mkl-2021.4.0         | 114.9 MB  | ###6       |  36% \n",
      "mkl-2021.4.0         | 114.9 MB  | ####       |  41% \n",
      "mkl-2021.4.0         | 114.9 MB  | ####5      |  45% \n",
      "mkl-2021.4.0         | 114.9 MB  | ####9      |  50% \n",
      "mkl-2021.4.0         | 114.9 MB  | #####5     |  55% \n",
      "mkl-2021.4.0         | 114.9 MB  | ######1    |  61% \n",
      "mkl-2021.4.0         | 114.9 MB  | ######5    |  66% \n",
      "mkl-2021.4.0         | 114.9 MB  | #######2   |  72% \n",
      "mkl-2021.4.0         | 114.9 MB  | #######6   |  77% \n",
      "mkl-2021.4.0         | 114.9 MB  | ########1  |  81% \n",
      "mkl-2021.4.0         | 114.9 MB  | ########6  |  87% \n",
      "mkl-2021.4.0         | 114.9 MB  | #########1 |  91% \n",
      "mkl-2021.4.0         | 114.9 MB  | #########6 |  97% \n",
      "mkl-2021.4.0         | 114.9 MB  | ########## | 100% \n",
      "\n",
      "numpy-1.21.5         | 25 KB     |            |   0% \n",
      "numpy-1.21.5         | 25 KB     | ########## | 100% \n",
      "numpy-1.21.5         | 25 KB     | ########## | 100% \n",
      "\n",
      "ca-certificates-2022 | 189 KB    |            |   0% \n",
      "ca-certificates-2022 | 189 KB    | 8          |   8% \n",
      "ca-certificates-2022 | 189 KB    | ########## | 100% \n",
      "ca-certificates-2022 | 189 KB    | ########## | 100% \n",
      "\n",
      "certifi-2022.9.14    | 156 KB    |            |   0% \n",
      "certifi-2022.9.14    | 156 KB    | ########## | 100% \n",
      "certifi-2022.9.14    | 156 KB    | ########## | 100% \n",
      "\n",
      "conda-4.14.0         | 1.0 MB    |            |   0% \n",
      "conda-4.14.0         | 1.0 MB    | ########## | 100% \n",
      "conda-4.14.0         | 1.0 MB    | ########## | 100% \n",
      "\n",
      "hdbscan-0.8.28       | 496 KB    |            |   0% \n",
      "hdbscan-0.8.28       | 496 KB    | ########## | 100% \n",
      "hdbscan-0.8.28       | 496 KB    | ########## | 100% \n",
      "\n",
      "openssl-1.1.1q       | 5.8 MB    |            |   0% \n",
      "openssl-1.1.1q       | 5.8 MB    | #6         |  16% \n",
      "openssl-1.1.1q       | 5.8 MB    | #######7   |  77% \n",
      "openssl-1.1.1q       | 5.8 MB    | ########## | 100% \n",
      "\n",
      "numpy-base-1.21.5    | 4.4 MB    |            |   0% \n",
      "numpy-base-1.21.5    | 4.4 MB    | #8         |  19% \n",
      "numpy-base-1.21.5    | 4.4 MB    | ########## | 100% \n",
      "numpy-base-1.21.5    | 4.4 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec6cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
